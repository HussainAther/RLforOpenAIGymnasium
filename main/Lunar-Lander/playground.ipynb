{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a5cc7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26.1\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "print(gym.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f5f1a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/syedather/opt/anaconda3/envs/orel/bin/python3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfba592d-e412-446f-bbfe-b9182ca1cc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLGlue:\n",
    "    \"\"\"RLGlue class\n",
    "    args:\n",
    "        env_name (string): the name of the module where the Environment class can be found\n",
    "        agent_name (string): the name of the module where the Agent class can be found\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env_class, agent_class):\n",
    "        self.environment = env_class()\n",
    "        self.agent = agent_class()\n",
    "\n",
    "        self.total_reward = None\n",
    "        self.last_action = None\n",
    "        self.num_steps = None\n",
    "        self.num_episodes = None\n",
    "\n",
    "    def rl_init(self, agent_init_info={}, env_init_info={}):\n",
    "        \"\"\"Initial method called when RLGlue experiment is created\"\"\"\n",
    "        self.environment.env_init(env_init_info)\n",
    "        self.agent.agent_init(agent_init_info)\n",
    "\n",
    "        self.total_reward = 0.0\n",
    "        self.num_steps = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def rl_start(self, agent_start_info={}, env_start_info={}):\n",
    "        \"\"\"Starts RLGlue experiment\n",
    "        Returns:\n",
    "            tuple: (state, action)\n",
    "        \"\"\"\n",
    "\n",
    "        last_state = self.environment.env_start()\n",
    "        self.last_action = self.agent.agent_start(last_state)\n",
    "\n",
    "        observation = (last_state, self.last_action)\n",
    "\n",
    "        return observation\n",
    "\n",
    "    def rl_agent_start(self, observation):\n",
    "        \"\"\"Starts the agent.\n",
    "        Args:\n",
    "            observation: The first observation from the environment\n",
    "        Returns:\n",
    "            The action taken by the agent.\n",
    "        \"\"\"\n",
    "        return self.agent.agent_start(observation)\n",
    "\n",
    "    def rl_agent_step(self, reward, observation):\n",
    "        \"\"\"Step taken by the agent\n",
    "        Args:\n",
    "            reward (float): the last reward the agent received for taking the\n",
    "                last action.\n",
    "            observation : the state observation the agent receives from the\n",
    "                environment.\n",
    "        Returns:\n",
    "            The action taken by the agent.\n",
    "        \"\"\"\n",
    "        return self.agent.agent_step(reward, observation)\n",
    "\n",
    "    def rl_agent_end(self, reward):\n",
    "        \"\"\"Run when the agent terminates\n",
    "        Args:\n",
    "            reward (float): the reward the agent received when terminating\n",
    "        \"\"\"\n",
    "        self.agent.agent_end(reward)\n",
    "\n",
    "    def rl_env_start(self):\n",
    "        \"\"\"Starts RL-Glue environment.\n",
    "        Returns:\n",
    "            (float, state, Boolean): reward, state observation, boolean\n",
    "                indicating termination\n",
    "        \"\"\"\n",
    "        self.total_reward = 0.0\n",
    "        self.num_steps = 1\n",
    "\n",
    "        this_observation = self.environment.env_start()\n",
    "\n",
    "        return this_observation\n",
    "\n",
    "    def rl_env_step(self, action):\n",
    "        \"\"\"Step taken by the environment based on action from agent\n",
    "        Args:\n",
    "            action: Action taken by agent.\n",
    "        Returns:\n",
    "            (float, state, Boolean): reward, state observation, boolean\n",
    "                indicating termination.\n",
    "        \"\"\"\n",
    "        ro = self.environment.env_step(action)\n",
    "        (this_reward, _, terminal) = ro\n",
    "\n",
    "        self.total_reward += this_reward\n",
    "\n",
    "        if terminal:\n",
    "            self.num_episodes += 1\n",
    "        else:\n",
    "            self.num_steps += 1\n",
    "\n",
    "        return ro\n",
    "\n",
    "    def rl_step(self):\n",
    "        \"\"\"Step taken by RLGlue, takes environment step and either step or\n",
    "            end by agent.\n",
    "        Returns:\n",
    "            (float, state, action, Boolean): reward, last state observation,\n",
    "                last action, boolean indicating termination\n",
    "        \"\"\"\n",
    "\n",
    "        (reward, last_state, term) = self.environment.env_step(self.last_action)\n",
    "\n",
    "        self.total_reward += reward\n",
    "\n",
    "        if term:\n",
    "            self.num_episodes += 1\n",
    "            self.agent.agent_end(reward)\n",
    "            roat = (reward, last_state, None, term)\n",
    "        else:\n",
    "            self.num_steps += 1\n",
    "            self.last_action = self.agent.agent_step(reward, last_state)\n",
    "            roat = (reward, last_state, self.last_action, term)\n",
    "\n",
    "        return roat\n",
    "\n",
    "    def rl_cleanup(self):\n",
    "        \"\"\"Cleanup done at end of experiment.\"\"\"\n",
    "        self.environment.env_cleanup()\n",
    "        self.agent.agent_cleanup()\n",
    "\n",
    "    def rl_agent_message(self, message):\n",
    "        \"\"\"Message passed to communicate with agent during experiment\n",
    "        Args:\n",
    "            message: the message (or question) to send to the agent\n",
    "        Returns:\n",
    "            The message back (or answer) from the agent\n",
    "        \"\"\"\n",
    "\n",
    "        return self.agent.agent_message(message)\n",
    "\n",
    "    def rl_env_message(self, message):\n",
    "        \"\"\"Message passed to communicate with environment during experiment\n",
    "        Args:\n",
    "            message: the message (or question) to send to the environment\n",
    "        Returns:\n",
    "            The message back (or answer) from the environment\n",
    "        \"\"\"\n",
    "        return self.environment.env_message(message)\n",
    "\n",
    "    def rl_episode(self, max_steps_this_episode):\n",
    "        \"\"\"Runs an RLGlue episode\n",
    "        Args:\n",
    "            max_steps_this_episode (Int): the maximum steps for the experiment to run in an episode\n",
    "        Returns:\n",
    "            Boolean: if the episode should terminate\n",
    "        \"\"\"\n",
    "        is_terminal = False\n",
    "\n",
    "        self.rl_start()\n",
    "\n",
    "        while (not is_terminal) and ((max_steps_this_episode == 0) or\n",
    "                                     (self.num_steps < max_steps_this_episode)):\n",
    "            rl_step_result = self.rl_step()\n",
    "            is_terminal = rl_step_result[3]\n",
    "\n",
    "        return is_terminal\n",
    "\n",
    "    def rl_return(self):\n",
    "        \"\"\"The total reward\n",
    "        Returns:\n",
    "            float: the total reward\n",
    "        \"\"\"\n",
    "        return self.total_reward\n",
    "\n",
    "    def rl_num_steps(self):\n",
    "        \"\"\"The total number of steps taken\n",
    "        Returns:\n",
    "            Int: the total number of steps taken\n",
    "        \"\"\"\n",
    "        return self.num_steps\n",
    "\n",
    "    def rl_num_episodes(self):\n",
    "        \"\"\"The number of episodes\n",
    "        Returns\n",
    "            Int: the total number of episodes\n",
    "        \"\"\"\n",
    "        return self.num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40e293b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic packages\n",
    "import os\n",
    "\n",
    "import shutil\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Lunar lander and RL-GLUE packages\n",
    "from environment import BaseEnvironment\n",
    "\n",
    "from lunar_lander import LunarLanderEnvironment\n",
    "\n",
    "from agent import BaseAgent\n",
    "\n",
    "from plot_script import plot_result, draw_neural_net\n",
    "\n",
    "# Pytorch packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Gym packages\n",
    "import gym\n",
    "from gym import wrappers\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5204867f-a75f-464a-a8d2-338ddee5e1ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Take a random action\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m---> 12\u001b[0m     next_observation, reward, done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Collect observation, action, reward, and done flag\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobservation\u001b[39m\u001b[38;5;124m'\u001b[39m: observation, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m'\u001b[39m: action, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m'\u001b[39m: reward, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdone\u001b[39m\u001b[38;5;124m'\u001b[39m: done})\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "# Create LunarLander-v2 environment\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "# Run an episode to collect data\n",
    "observation = env.reset()\n",
    "done = False\n",
    "data = []\n",
    "\n",
    "while not done:\n",
    "    # Take a random action\n",
    "    action = env.action_space.sample()\n",
    "    next_observation, reward, done = env.step(action)\n",
    "    \n",
    "    # Collect observation, action, reward, and done flag\n",
    "    data.append({'observation': observation, 'action': action, 'reward': reward, 'done': done})\n",
    "    \n",
    "    observation = next_observation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
